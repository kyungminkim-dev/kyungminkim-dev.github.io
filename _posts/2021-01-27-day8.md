---
title:  "Day8 학습정리"
excerpt: "AI Math"

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git]
use_math: true
toc: true
toc_sticky: true
 
date: 2021-01-27
last_modified_at: 2021-02-07
---


## day8 학습정리

* 비선형모델 : 신경망(neural network)

    * 신경망은 선형모델과 활성함수를 합성한 함수이다. 

    * $O = XW + b$  $(O:n*p, X:n*d, W:d*p,b:n*p)$

        * 데이터가 바뀌면 결과값도 바뀌게 된다. 이 때 출력 벡터의 차원은 $d$ 에서 $p$로 바뀌게 된다.

        * $d$개의 변수로 $p$개의 잠재변수를 설명하는 모델을 상상할 수 있다.


    * 신경망은 입력층, 은닉층, 출력층으로 구성되어 있다. 

    * x1,x2이라는 신호를 입력받아 y를 출력하는 퍼셉트로은 다음과 같다.
        

        $b + w_{1}*x_{1} + w_{2}*x_{2} = y$


        여기서 b는 편향(bias) w는 가중치(weight)라고 한다.


---


* softmax function  

    * 소프트맥스 함수는 모델의 출력을 **확률**로 해석할 수 있게 변환해주는 연산이다. 분류문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측한다. 

    * 출력벡터 $O$에 softmax 함수를 합성하면 확률벡터가 되므로 특정 클래스 $k$에 속할 확률로 해석할 수 있다.

    * $softmax(O) = softmax(Wx+b)$


    ```
    def softmax(vec):
        denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))
        numerator = np.sum(denumerator, axis= -1, keepdims=True)
        val = denumerator / numerator
        return val
    ```

    
    * 분류문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측한다.


---


*  활성함수란?

    * 활성함수는 $R$ 위에 정의된 비선형 함수이다. 활성함수를 사용하지 않으면 딥러닝은 선형모델과 차이가 없다.

    * 활성함수의 종류에는 시그모이드, $tanh$, $ReLU$함수등이 있다. 딥러닝에서는 $ReLU$ 함수를 많이 사용한다. 

        $H = (\sigma(z_1),...\sigma(z_n))$

        $\sigma(z) = \sigma(Wx+b)$
    
    * 활성함수 $\sigma$는 비선형함수로 잠재벡터 $z=(z_1,...z_q)$의 각 노드에 개별적으로 적용하여 새로운 잠재벡터 $H=(\sigma(z_1),...\sigma(z_n))$를 만들다. 

---


* 역전파 알고리즘(backpropagation)

    * 딥러닝은 역전파 알고리즘을 이용하여 각 층에 사용된 파라미터(w,b) (깊이 1~L) 를 학습한다.

    * 역전파는 L -> 1 순서로 연쇄법칙을 통해 그레디언트 벡터를 전달한다.

    * 역전파 알고리즘은 합성함수 미분법인 연쇄법칙(chain-rule) 기반 자동미분을 사용한다.



