---
title:  "Day13 학습정리"
excerpt: "DL Basic"

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git]
use_math: true
toc: true
toc_sticky: true
 
date: 2021-02-03
last_modified_at: 2021-02-12
---


## Day13 학습정리

### 여러가지 CNN

* AlexNet

    ![image-AlexNet](../../assets/img/boostcamp/AlexNet.png)

    * AlexNet은 2012년 ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 대회에서 우승한 CNN 구조이다.

        * Key ideas(왜 성공했을까?)

            * Rectified Linear Unit(ReLU) activation(Non-linear)의 사용. 0보다 클 때의 기울기가 1이기 때문에 layer가 깊어졌을때 발생할 수 있는 기울기 손실등을 막을 수 있다. 

            * GPU implementation(2 GPUs)

            * Local response normalization, Overlapping pooling

            * Data augmentation

            * Dropout

    * 네트워크가 두개로 나뉘어져있다. 2개의 GPU에 따로따로 training을 시킴. 

    * 5개의 Convolution layer와 3개의 Dense layer로 구성된다.

    * input에서 11 by 11 feature를 사용한다. 하나의 convolution filter가 볼 수 있는 이미지 영역의 크기가 크지만 파라미터 수를 너무 많이 필요로함. 
    
    
    * ReLU Activation

        ![image-ReLU](../../assets/img/boostcamp/ReLU.png)

        * preseves properties of linear models

        * Easy to optimize with gradient descent

        * Good generalization

        * Overcome the vanishing gradient problem

---

* VGGNet

    ![image-VGGNet](../../assets/img/boostcamp/VGGNet.png)

    * Increasing depth with 3 * 3 convolution filters(with stride 1)

    * 1 * 1 convolution for fully connected layers

    * Dropout (p=0.5)

    * VGG16, VGG19

    * Why $3 \times 3$ convolution ?  ==> $3 \times 3$ 으로 세번 컨볼루션 하는것이 $7 \times 7$로 한번 컨볼루션 하는것보다 가중치가 적게 든다. 

* GoogLeNet

    ![image-GoogLeNet](../../assets/img/boostcamp/GoogLeNet.png)

    * GoogLeNet won the ILSVRC at 2014 (It combined network-in-network(NiN) with inception blocks.) 

    * 22 layers , 9 Inecption Block

    ![image-InceptionBlock](../../assets/img/boostcamp/InceptionBlock.png)

        **Inception Block**
        
            What are the benefits of the inception block? ==> Reduce the number of parameter.

            How to reduce paramters? Recall how the number of parameters is computed.

            1 X 1 convolution can be seen as channel-wise dimension reduction. (채널의 개수를 줄여준다.)

            Benefit of 1X1 convolution : 1X1 convolution enables about 30% reduce of the number of parameters!

---

* ResNet

    * Deeper neural networks are hard to train.

        * Overfitting is usually caused by an exessive number of parameters. (training error는 줄어드는데 test error는 커지는것)

        * But, not in this case. (Overfitting은 아니지만 학습이 안되는 것)

            ![image-DeepError](../../assets/img/boostcamp/DeepError.png)

        * Add an identity map(skip connection)

            ![image-ResidualBlock](../../assets/img/boostcamp/ResidualBlock.png)

            * $H(x)$의 값 즉, $F(x)+x$를 최소화하는 것이 목적이다. $x$는 고정된 값이므로 $F(x)$를 최소화 하는것이 목적이 되고 $F(x)$가 0에 수렴하면 $H(x)$와 $x$가 같아지게 된다. 즉 입력과 출력의 값이 같아질수록 좋은 성능을 낸다고 말하는것 같다.


            * Batch normalization after convolutions

                ![image-BatchNormAfterConv](../../assets/img/boostcamp/BatchNormAfterConv.png)

                * batch normalization을 conv 후에 하는것과 ReLU후에 하는것의 성능차이에 대한 의견 대립이 있는것 같다.


* DenseNet

    * DenseNet uses concatenation instead of addition.

    * Dense Block 

        * Each layer concatenates the feature maps of all preceding layers.

        * The number of channels increase geometrically.

    * Transition Block

        * BatchNorm -> $1 \times 1$ Conv -> $2 \times 2$ AvgPooling

        * Dimenstion reduction

---

* Key takeaways

    * VGG : repeated $3 \times 3$ blocks

    * GooLeNet : $1 \times 1$ convolution 

    * ResNet : skip-connetion 

    * DenseNet : concatenation

