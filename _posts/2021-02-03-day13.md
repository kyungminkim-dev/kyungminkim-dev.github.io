---
title:  "Day13 학습정리"
excerpt: "DL Basic"

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git]
use_math: true
toc: true
toc_sticky: true
 
date: 2021-02-03
last_modified_at: 2021-02-12
---


## Day13 학습정리

* Convolution 

    * Continuous convolution

        $(f*g)(t) = \int f(\tau)g(t-\tau)d\tau = \int f(t-\tau)g(t)d\tau$

    * Discrete convolution
    
        $(f*g)(t) = \sum_{i=- \infty}^{\infty}f(i)g(t-i) = \sum_{i=- \infty}^{\infty}f(t-i)g(i)$

    * 2D image convolution

        $(I*K)(i,j) = \sum_{m}\sum_{n}I(m,n)K(i-m,j-n) = \sum_{m}\sum_{n}I(i-m,i-n)K(m,n)$

* Convolutional Neural Networks

    * CNN consists of convolution layer, pooling layer, and fully connected layer.

    * Convolution and pooling layers : feature extraction

        * 합성곱 신경망에서는 풀링 층을 사용해 표현의 크기를 줄임으로써 계산속도를 줄이고 특징을 더 잘 검출 해낼 수 있다.

        * parameter를 줄이기 때문에, 해당 network의 표현력이 줄어들어 overfitting을 억제.

        * max pooling, avg pooling 두가지 종류가 있다. 보통 max pooling을 사용 


    * Fully connected layer : decision making (e.g., classification)

        * 완전연결 계층은 한 층의 모든 뉴런이 다음 층의 모든 뉴런과 연결된 상태를 이야기 한다. 


### 여러가지 CNN

* AlexNet

    ![image-AlexNet](../../assets/img/boostcamp/AlexNet.png)

    * AlexNet은 2012년 ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 대회에서 우승한 CNN 구조이다.

        * Key ideas(왜 성공했을까?)

            * Rectified Linear Unit(ReLU) activation(Non-linear)의 사용. 0보다 클 때의 기울기가 1이기 때문에 layer가 깊어졌을때 발생할 수 있는 기울기 손실등을 막을 수 있다. 

            * GPU implementation(2 GPUs)

            * Local response normalization, Overlapping pooling

            * Data augmentation

            * Dropout

    * 네트워크가 두개로 나뉘어져있다. 2개의 GPU에 따로따로 training을 시킴. 

    * 5개의 Convolution layer와 3개의 Dense layer로 구성된다.

    * input에서 11 by 11 feature를 사용한다. 하나의 convolution filter가 볼 수 있는 이미지 영역의 크기가 크지만 파라미터 수를 너무 많이 필요로함. 
    
    
    * ReLU Activation

        ![image-ReLU](../../assets/img/boostcamp/ReLU.png)

        * preseves properties of linear models

        * Easy to optimize with gradient descent

        * Good generalization

        * Overcome the vanishing gradient problem

---

* VGGNet

    ![image-VGGNet](../../assets/img/boostcamp/VGGNet.png)

    * Increasing depth with 3 * 3 convolution filters(with stride 1)

    * 1 * 1 convolution for fully connected layers

    * Dropout (p=0.5)

    * VGG16, VGG19

    * Why $3 \times 3$ convolution ?  ==> $3 \times 3$ 으로 세번 컨볼루션 하는것이 $7 \times 7$로 한번 컨볼루션 하는것보다 가중치가 적게 든다. 

* GoogLeNet

    ![image-GoogLeNet](../../assets/img/boostcamp/GoogLeNet.png)

    * GoogLeNet won the ILSVRC at 2014 (It combined network-in-network(NiN) with inception blocks.) 

    * 22 layers , 9 Inecption Block

    ![image-InceptionBlock](../../assets/img/boostcamp/InceptionBlock.png)

        **Inception Block**
        
            What are the benefits of the inception block? ==> Reduce the number of parameter.

            How to reduce paramters? Recall how the number of parameters is computed.

            1 X 1 convolution can be seen as channel-wise dimension reduction. (채널의 개수를 줄여준다.)

            Benefit of 1X1 convolution : 1X1 convolution enables about 30% reduce of the number of parameters!

---

* ResNet

    * Deeper neural networks are hard to train.

        * Overfitting is usually caused by an exessive number of parameters. (training error는 줄어드는데 test error는 커지는것)

        * But, not in this case. (Overfitting은 아니지만 학습이 안되는 것)

            ![image-DeepError](../../assets/img/boostcamp/DeepError.png)

        * Add an identity map(skip connection)

            ![image-ResidualBlock](../../assets/img/boostcamp/ResidualBlock.png)

            * $H(x)$의 값 즉, $F(x)+x$를 최소화하는 것이 목적이다. $x$는 고정된 값이므로 $F(x)$를 최소화 하는것이 목적이 되고 $F(x)$가 0에 수렴하면 $H(x)$와 $x$가 같아지게 된다. 즉 입력과 출력의 값이 같아질수록 좋은 성능을 낸다고 말하는것 같다.


            * Batch normalization after convolutions

                ![image-BatchNormAfterConv](../../assets/img/boostcamp/BatchNormAfterConv.png)

                * batch normalization을 conv 후에 하는것과 ReLU후에 하는것의 성능차이에 대한 의견 대립이 있는것 같다.


* DenseNet

    * DenseNet uses concatenation instead of addition.

    * Dense Block 

        * Each layer concatenates the feature maps of all preceding layers.

        * The number of channels increase geometrically.

    * Transition Block

        * BatchNorm -> $1 \times 1$ Conv -> $2 \times 2$ AvgPooling

        * Dimenstion reduction

---

* Key takeaways

    * VGG : repeated $3 \times 3$ blocks

    * GooLeNet : $1 \times 1$ convolution 

    * ResNet : skip-connetion 

    * DenseNet : concatenation


<br><br/>

## Semantic Segmentation
---
원본 이미지를 의미 있는 부분으로 묶어 분할하는 방법. 픽셀 단위로 분류를 진행한다. Semantic Segmentation은 이미지의 전체 픽셀을 레이블로 분류하기 때문에 전체 이미지를 하나의 레이블로 분류하는 단순 이미지 분류 보다 더 어렵다.<br/>
Semantic Segmentation은 크게 세가지 과정으로 표현된다.<br/>
1. Convolutionalization
2. Deconvolution(Upsampling)
3. Skip architecture


<br/>

* Convolutionalization

    ![image-Convolutionalization](../../assets/img/boostcamp/Convolutionalization.png)

    Semantic Segmentation은 기존의 CNN의 FC층을 $1 \times\ 1$ Convolution층으로 변경한다.<br/>
    기존 FC층을 이용하면 위치정보가 사라지지만 convolution층을 이용하면 위치정보를 유지한 채 인풋 이미지에 대한 픽셀별 레이블링 결과(Heat Map)를 출력한다. 

* Deconvolution(Upsampling)

    축소된 크기의 아웃풋(Heat Map)을 원본 이미지 크기로 확대한다. 원본이미지의 하나의 값을 필터 부분에 대응되는 값과 모두 곱해서 아웃풋 이미지의 필터 크기 값을 출력한다. 따라서 디컨볼루션의 결과 차원이 확장된다.


* skip architecture

---

## Detection

* R-CNN

* SPPNet

* Fast R-CNN

* Faster R-CNN
    
