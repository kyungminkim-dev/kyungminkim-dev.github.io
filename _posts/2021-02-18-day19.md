---
title:  "Day19 학습정리(Transformer)"
excerpt: "AI Math"

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git]
use_math: true
toc: true
toc_sticky: true
 
date: 2021-02-18
last_modified_at: 2021-03-01
---

1. Transformer

2. Transformer (cont'd)

--- 

## Transformer 

* RNN 또는 CNN 모듈없이 Attention만을 사용해 sequence데이터를 입력으로 받고 예측함

![image-Transformer_Graph](../../assets/img/boostcamp/Transformer_Graph.png)

* 자기 자신과의 내적, 나머지 input과의 내적을 통해 $h_{i}$를 만들어 사용. 인코더의 hidden state vector와 디코더의 hidden state vector의 구별이 없어짐.(Self-Attention)

    * 자기 자신과의 내적을 하면 자신과의 내적값이 크게 나오게 된다. 

    * Query, Key, Value를 이용해 해결 

    * 임베딩 벡터의 차원(단어의 차원)은 $W_{Q}$의 행의 갯수와 값고, $W_{Q}$의 열은 임의로 지정?


![image-Transformer_2](../../assets/img/boostcamp/Transformer_2.png)


![image-Transformer_3](../../assets/img/boostcamp/Transformer_3.png)


![image-Transformer_4](../../assets/img/boostcamp/Transformer_4.png)


![image-Transformer_5](../../assets/img/boostcamp/Transformer_5.png)

---

## Transformer : Multi-Head Attention

![image-Multi_Head_Attention](../../assets/img/boostcamp/Multi_Head_Attention.png)
