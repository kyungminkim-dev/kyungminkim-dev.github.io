---
title:  "Day7 학습정리"
excerpt: "AI Math"
permalink: /BoostcampAI/
categories: [BoostcampAI]
tags:
  - [Blog, jekyll, Github, Git]
use_math: true
toc: true
toc_sticky: true
 
date: 2021-01-26
last_modified_at: 2021-02-07
---

## day7 학습정리

* 미분 

    * 미분은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구로 최적화에서 제일 많이 사용하는 기법.
    
    * 한 점에서 접선의 기울기를 알면 어느 방향으로 점을 움직여야 함수값이 증가하는지 감소하는지 알 수 있다.

    * 미분값을 더하면 경사상승법(gradient ascent)이라 하며 함수의 극대값의 위치를 구할 때 사용한다.

    * 미분값을 빼면 경사하강법(gradient descent)이라 하며 함수의 극소값을 구할 때 사용한다.
    
    

        * 경사하강법 알고리즘 함수가 ($f(x) = x^2 + 2*x + 3$ 일때)
        
        
        ```
        # 경사하강법을 이용하여 미분값이 0이 되는 지점을 찾는다. 
        # 컴퓨터로 계산할 때 미분이 정확히 0이 되는 것은 불가능하므로 epsilon 보다 작을 때 종료하는 조건이 필요하다.
        import sympy as sym
        from sympy.abc import x
        import numpy as np

        def func(val):
            fun = sym.poly(x**2 + 2*x + 3)
            return fun.subs(x,val), fun

        def func_gradient(fun,val):
            _, function = func(val)
            diff = sym.diff(function, x)
            return diff.subs(x , val) , diff

        def gradient_descent(fun, init_point, lr_rate = 1e-2, epsilon = 1e-5):
            cnt = 0
            val = init_point    
            diff, _ = func_gradient(fun, init_point)
            while abs(diff) > epsilon: #종료조건이 성립할때까지 미분값을 업데이트 한다.
                val -= lr_rate * diff
                diff, _ = func_gradient(fun, val)
                cnt += 1
            print(f"함수 : {fun(val)[1]}, 연산횟수 : {cnt} , 최소점 : ({val} , {fun(val)[0]})")

        gradient_descent(fun = func, init_point = np.random.uniform(-2,2))
        ```



    * 벡터가 입력인 다변수 함수의 경우 편미분을 사용한다. 각 변수별로 편미분을 계산한 그레디언트(gradient)벡터를 이용하여 경사하강/경사상승법에 사용한다.

        * $\nabla f = (\partial_{x_{1}}f,\partial_{x_{2}}f,...,\partial_{x_{d}}f)$
        
        

        * 함수가 $f(x) = x^2 + 2y^2$ 일 때  경사하강법으로 최소점을 찾는 코드
        
        
        ```
        import sympy as sym
        from sympy.abc import x ,y
        import numpy as np

        def eval_(fun,val):
            val_x, val_y = val
            fun_eval = fun.subs(x, val_x).subs(y, val_y)
            return fun_eval

        def func_multi(val):
            x_, y_ = val
            func = sym.poly(x**2 + 2*y**2)
            return eval_(func, [x_,y_]), func

        def func_gradient(fun, val):
            x_, y_ = val
            _, function = fun(val)
            diff_x = sym.diff(function, x)
            diff_y = sym.diff(function, y)
            grad_vec = np.array([eval_(diff_x, [x_,y_]), eval_(diff_y, [x_,y_])], dtype = float)
            return grad_vec, [diff_x, diff_y]

        def gradient_descent(fun, init_point, lr_rate = 1e-2, epsilon = 1e-5):
            cnt = 0
            val = init_point
            diff, _ = func_gradient(fun, val)
            while np.linalg.norm(diff) > epsilon:
                val -= lr_rate * diff
                diff, _ = func_gradient(fun, val)
                cnt += 1

            print(f"함수 : {fun(val)[1]}, 연산횟수 : {cnt}, 최소점 : ({val} , {fun(val)[0]})")

        pt = [np.random.uniform(-2,2), np.random.uniform(-2,2)]
        gradient_descent(fun = func_multi, init_point = pt)
        ```



* 확률적 경사하강법(Stochastic Gradient Descent)

    * 이론적으로 경사하강법은 미분가능하고 볼록한 함수에 대해선 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장되어 있다. 하지만 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지는 않는다.

    * 확률적 경사하강법은 모든 데이터를 사용해서 업데이트하는 대신 데이터 한개 또는 일부를 활용하여 업데이트를 진행한다. 볼록이 아닌(non-convex) 목적식은 SGD를 통해 최적화할 수 있다.

    * 이 방법에서는 loss function을 계산할 때 전체 데이터 대신 일부 데이터의 모음(mini-batch)에 대해서만 loss function을 계산.

    * Batch gradient descent 보다 다소 부정확할 수는 있지만, 훨씬 계산 속도가 빠르기 때문에 같은 시간 더 많은 step을 갈 수 있으며 여러 번 반복할 경우 보통 batch의 결과가 유사한 경과로 수렴한다. 또한, SGD를 사용할 경우 batch gradient descent에서 빠질 local minima에 빠지지 않고 더 좋은 방향으로 수렴할 가능성도 있다.


    ```
    import numpy as np

    X = np.array([[1,1],[1,2],[2,2],[2,3]])
    y = np.dot(X, np.array([1,2])) + 3

    beta_gd = [10.1, 15.1, -6.5]
    X_ = np.array([np.append(x,[1]) for x in X])

    for t in range(5000): #경사하강법 알고리즘에선 학습률과 학습횟수가 중요한 변수가 된다.
        error = y - X_ @ beta_gd
        # error = error / np.linalg.norm(error)
        grad = - np.transpose(X_) @ error
        beta_gd = beta_gd - 0.01 * grad

    print(beta_gd)
    ```
    
