---
title:  "Day18 학습정리"
excerpt: "AI Math"

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git]
use_math: true
toc: true
toc_sticky: true
 
date: 2021-02-17
last_modified_at: 2021-02-22
---

1. Seq2Seq with attention

2. Endoder-decoder architecture

3. Attention mechanism

4. Beam search

5. BLEU score

---

## Seq2seq with attention

* It takes a sequence of words as input and gives a sequence of words as output.

* It composed if an encoder and a decoder

* 시퀀스 투 시퀀스 모델은 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델이다. 챗봇, 기계번역 등에 사용된다. 

    ![image-s2s](../../assets/img/boostcamp/s2s.png)

    seq2seq은 각 time step 마다 output이 나오기 때문에 RNN의 모형중 many to many 타입에 속한다.

* Attention 

    기존의 RNN을 사용한 seq2seq은 기울기 손실문제와 정보의 손실문제가 발생한다. 이를 해결하기 위해 attention 기법이 추가됐다.

    * Attention provides a solution to the bottleneck problem.

    * Core idea : At each time step of the decoder, focus on a particular part of the source sequence.<br/>
    Attention의 기본 아이디어는 디코더에서 매 time step마다 인코더에서 전체 입력문장을 참고한다. 해당 시점에서 예측해야 할 단어와 연관이 있는 단어에 집중한다. encoder의 time step별 $h_{t}^{(e)}$를 참고하여 예측. $h_{t}^{(d)}$ 와 $h_{t}^{(e)}$의 내적을 통하여 softmaxt함수에 통과시켜 Attention distribution을 만들어냄. 이를 Attention module이라 함.

    * Attention module의 input은 encoder hidden state vector와 decoder hidden state vector. Attention의 종류에는 여러가지가 있다.

    * Use the attention distribution to take a weighted sum of the encoder hidden states

    * The attention output mostly contains information the hidden states that received high attenion

    * Concatenate attention output with decoder hidden state, then use to compute $\hat{y_{1}}$ as before

        ![image-ConcateAttetion](../../assets/img/boostcamp/ConcateAttetion.png)

        * encoder의 hidden state vector가 $h_{0}^{(d)}$로 들어감. $h_{0}^{(d)}$와 $x_{1}$이 $h_{1}^{(d)}$로 들어가 단어를 예측. ??<br/>
        
    
    * Attention의 종류 

        ![image-TypeAttention](../../assets/img/boostcamp/TypeAttention.png)

---

* Attention의 장점

    * Attention significantly improves NMT performance.

        * It is useful to allow the decoder to focus on particular parts of the source.

    * Attention solves the bottleneck problem.

        * Attention allows the decoder to look directly at source; bypass the bottleneck.

    * Attention helps with vanishing gradient problem.

        * Provides a shortcut to far-away states.

    * Atteion provides some interpretability.

        * By inspecting attention distribution, we can see what the decoder was focusing on 

        * The network just alignment by itself

---

## Beam search

* Greedy decoding 

    * Greedy decoding has no way to undo decision. (예측 단어가 틀리면 돌아갈 수 없음)

* Exhausive search

    * Ideally, we want to find a (length T) translation y that maximizes<br/>

        $P(y|x)=_{1}\mathrm{\Pi}_{T}p(y_t|y_1,...,y_{t-1},x)$

    * We could try computing all possible sequences y

        * This means that on each step $t$ of the decoder, we are tracking $V^{t}$ possible partial translations, where $V$ is the vocabulary size

        * This $O(V^t)$ complexity is fat too expensive!

* Beam search

    * Core idea : on each time step of the decoder, we keep track of the $k$ most probable partial translations (which we call hypothese)

        * $k$ is the beam size (in practice around 5 to 10)
    
    * A hypothesis $y_1,...,y_t$ has a score of its log probability

        * Scores are all negative, and a higher score is better.

        * We search for high-scoring hypotheses, tracking the top $k$ ones on each step

    * Beam search is not guaranteed to find a globally optimal solution.

    * But it is much more efficient than exhausive search.


    ![image-BeamSearch](../../assets/img/boostcamp/BeamSearch.png)


---

### Beam search : Stopping criterion

* In greedy decoding, usually we decode until the model produces a <\END> token

    * For example : <\START> he hit me with a pie <\END>

* In beam search decoding, different hypotheses may produce <\END> tokens on different timesteps

    * When hypothesis produces <\END>, that hypothesis is complete

    * Place it aside and continue exploring other hypotheses via beam search.

* Usually we continue beam search until:

    * We reach timestep $T$ (where $T$ is some pre-defined cutoff) or

    * We have at least $n$ completed hypotheses (where $n$ is the pre-defined cutoff)


---

### BLEU score

* 언어 모델의 성능을 측정하기 위한 방법은 perplexity, BLEU등이 있다.

* BLEU는 기계 번역 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 성능을 측정한다. 측정 기준은  n-gram으로 한다.

![image-PrecisionRecall](../../assets/img/boostcamp/PrecisionRecall.png)


![image-BLEU](../../assets/img/boostcamp/BLEU.png)

    











