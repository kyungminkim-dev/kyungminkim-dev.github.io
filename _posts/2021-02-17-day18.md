---
title:  "Day18 학습정리"
excerpt: "AI Math"

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git]
use_math: true
toc: true
toc_sticky: true
 
date: 2021-02-17
last_modified_at: 2021-02-18
---

# Day18 학습정리

1. Seq2Seq with attention

2. Endoder-decoder architecture

3. Attention mechanism

4. Beam search

5. BLEU score

---

## Seq2seq with attention

* It takes a sequence of words as input and gives a sequence of words as output.

* It composed if an encoder and a decoder

* 시퀀스 투 시퀀스 모델은 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델이다. 챗봇, 기계번역 등에 사용된다. 

    ![image-s2s](../../assets/img/boostcamp/s2s.png)

    seq2seq은 각 time step 마다 output이 나오기 때문에 RNN의 모형중 many to many 타입에 속한다.

* Attention 

    기존의 RNN을 사용한 seq2seq은 기울기 손실문제와 정보의 손실문제가 발생한다. 이를 해결하기 위해 attention 기법이 추가됐다.

    * Attention provides a solution to the bottleneck problem.

    * Core idea : At each time step of the decoder, focus on a particular part of the source sequence.<br/>
    Attention의 기본 아이디어는 디코더에서 매 time step마다 인코더에서 전체 입력문장을 참고한다. 해당 시점에서 예측해야 할 단어와 연관이 있는 단어에 집중한다. encoder의 time step별 $h_{t}^{(e)}$를 참고하여 예측. $h_{t}^{(d)}$ 와 $h_{t}^{(e)}$의 내적을 통하여 softmaxt함수에 통과시켜 Attention distribution을 만들어냄. 이를 Attention module이라 함.

    * Attention module의 input은 encoder hidden state vector와 decoder hidden state vector. Attention의 종류에는 여러가지가 있다.

    * Use the attention distribution to take a weighted sum of the encoder hidden states

    * The attention output mostly contains information the hidden states that received high attenion

    * Concatenate attention output with decoder hidden state, then use to compute $\hat{y_{1}}$ as before

        ![image-ConcateAttiention](../../assets/img/boostcamp/ConcateAttiention.png)

        * encoder의 hidden state vector가 $h_{0}^{(d)}$로 들어감. $h_{0}^{(d)}$와 $x_{1}$이 $h_{1}^{(d)}$로 들어가 단어를 예측. ??<br/>
        
    
    * Attention의 종류 

        ![image-TypeAttention](../../assets/img/boostcamp/TypeAttention.png)




