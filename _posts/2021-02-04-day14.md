---
title:  "Day14 학습정리"
excerpt: "DL Basic"

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git]
use_math: true
toc: true
toc_sticky: true
 
date: 2021-02-04
last_modified_at: 2021-02-14
---

# day14 학습정리
## [AI Math 10강] RNN 첫걸음

**시퀀스 데이터의 이해**

* 소리, 문자열, 주가 등의 데이터를 시퀀스(sequence) 데이터로 분류한다. 

* 시계열(time-series) 데이터는 시간 순서에 따라 나열된 데이터로 시퀀스 데이터에 속한다. 

* 시퀀스 데이터는 독립동등분포(i.i.d.) 가정을 잘 위배하기 때문에 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다. 

* 과거 정보 또는 앞뒤 맥락 없이 미래를 예측하거나 문장을 완성하는 것은 불가능 하다. 

<br><br/>

**시퀀스 데이터를 다루는 법**

* 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할 수 있다.<br/>
    $P(X_1,...,X_t)=P(X_{t}|X_1,...,X_{t-1})P(X_1,...,X_{t-1})=P(X_{t}|X_1,...,X_{t-1})P(X_{t-1}|X_1,...,X_{t-2})P(X_1,...,X_{t-2}) = \prod_{s=1}^{t}P(X_{s}|X_{s-1},...,X_{1})$<br/>

    $X_{t} \sim P(X_t|X_{t-1},...,X_1)$
    
        
    * 위 조건부확률은 과거의 모든 정보를 사용하지만 시퀀스 데이터를 분석할 때 모든 과거 정보들이 필요한 것은 아니다.

* 시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요하다.
    
    * 고정된 길이 $\tau$만큼의 시퀀스만 사용하는 경우 AR($\tau$) (Autoregressive Model) 자기회귀모델이라고 부른다. 

    * 또 다른 방법은 바로 이전 정보를 제외한 나머지 정보들을 $H_t$라는 잠재변수로 인코딩해서 활용하는 잠재 AR모델이다.

    * 잠재변수 $H_t$를 신경망을 통해 반복해서 사용하여 시퀀스 데이터의 패턴을  학습하는 모델이 RNN이다.
    
    * $X_{t} \sim P(X_t|X_{t-1},...,X_1)$
    

    * $X_{t+1} \sim P(X_{t+1}|X_{t},X_{t-1},...,X_1)$
    
    
    * $H_{t} = Net_{\theta}(H_{t-1},X_{t-1})$
    
<br><br/>

---

**Recurrent Neural Network의 이해**

* 가장 기본적인 RNN 모형은 MLP와 유사한 모양이다.

    * $W^{(1)}, W^{(2)}$은 시퀀스와 상관없이 불변이 행렬이다.<br/>

        $O_{t} = H_{t}W^{(2)} + b^{(2)}$<br/>

        $H_{t} = \sigma{(X_{t}W_{x}^{(1)} +H_{t-1}W_{H}^{(1)}+b^{(1)}})$  ($H$ : 잠재변수, $\sigma$ : 활성화함수, $W$ : 가중치행렬, $b$ : bias)

* RNN은 이전 순서의 잠재변수와 현재의 입력을 활용하여 모델링합니다.

* RNN의 역전파는 잠재변수의 연결그래프에 따라 순차적으로 계산한다. 이를 Backpropagation Through Time(BPTT)라 한다.




**기울기 소실의 해결책**

* 시퀀스 길이가 길어지는 경우 BPTT를 통한 역전파 알고리즘의 계산이 불안정해지므로 길이를 끊는 것이 필요하다. 이를 truncated BPTT라 한다.

* 이런 문제들 때문에 Vanilla RNN은 길이가 긴 시퀀스 처리하는데 문제가 있다. 이를 해결하기 위해 등장한 RNN네트워크가 LSTM 과 GRU이다. 


---


## [DLBasic] Sequential Models - RNN

**Sequential Model**

* Naive sequence model

    Input $x_{t-2} \longrightarrow x_{t-1} \longrightarrow x_{t}$<br/>

    $P(x_{t}|x_{t-1},x_{t-2},...)$

    $x_{t-1}, x_{t-2}...$ : The number of inputs varies.  
    
* Latent autogressive model

  $\hat{x} = p(x_t|h_t)$

  $h_t=g(h_{t-1},x_{t-1})$

  $h$ : summary of the past

    
