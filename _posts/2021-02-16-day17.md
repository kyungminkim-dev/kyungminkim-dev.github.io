---
title:  "Day17 학습정리"
excerpt: "DL Basic"

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git]
use_math: true
toc: true
toc_sticky: true
 
date: 2021-02-16
last_modified_at: 2021-02-21
---

## 1.Basics of Recurrent Neural Networks(RNN)

### Recurrent Neural Networks

* Input and outputs of RNNS(rolled version)

    * We usually want to predict a vector as some time steps

![image-RolledRNN](../../assets/img/boostcamp/RolledRNN.png)

* How to calculate the hidden state of RNNs

    * We can process a sequence of vectors by applying a recurrence formula at every time step

    * $h_{t-1}$  :  old hidden-state vector

    * $x_t$  :  input vector at some time step 

    * $h_t$  :  new hidden-state vector

    * $f_W$  :  RNN function with parameters $W$ 

    * $y_t$  :  output vector at time step $t$

    * Notice : The same function and the same set of parameters are used at every time step
    
    * The state consists of a single 'hidden' vector h<br/>

        $h_{t}=f_{W}(h_{t-1},x_{t}) \longrightarrow h_{t}=tanh(W_{hh}h_{t-1}+W_{xh}x_{t})$<br/>


        $y_{t}=W_{hy}h_{t}$<br/>

---

### Type of RNN

![image-TypeOfRNN](../../assets/img/boostcamp/TypeOfRNN.png)

* One-to-One : Standard Neural Networks

* One-to-many : Image Captioning

* Many-to-one : Sentiment Classification

* Sequence-to-sequence : Machine Translation, Video classification on frame level

---

## Character-level Language Model

* Example of training sequence "hello"

![image-TrainingHello](../../assets/img/boostcamp/TrainingHello.png)

* Volcabulary : [h,e,l,o]

* $h_{t}=tanh(W_{hh}h_{t-1}+W_{xh}x_{t}+b$

* $Logit=W_{hy}h_{t}+b$

![image-TestHello](../../assets/img/boostcamp/TestHello.png)

* At test-time, sample characters one at a time, feed back to model

---

### Vanishing/Exploding Gradient Problem in RNN

* Multiplying the same matrix at each time step during backpropagation causes gradient vanishing or exploding.<br/>

Solution : LSTM, GRU

### LSTM

* Core Idea : pass cell state information straightly without any transformation : Solving long-term dependency problem

* A gate exists for controlling how much information could flow from cell state.

![image-CellState](../../assets/img/boostcamp/CellState.png)

* Forget gate

    * $f_{t}=\sigma(W_{f} \cdot[h_{t-1},x_{t}]+b_{f})$<br/>

    ![image-ForgetGate](../../assets/img/boostcamp/ForgetGate.png)

* Generate information to be added and cut it by input gate

    * $i_{t}=\sigma(W_{i} \cdot[h_{t-1},x_{t}]+b_{i})$<br/>

    * $\tilde{C_t}=tanh(W_{C} \cdot[h_{t-1},x_{t}]+b_{C})$

* Generate new cell state by adding current information to previous cell state 

    * $C_{t}=f_{t}\cdot C_{t-1}+i_{t}\cdot \tilde{C_{t}}$

![image-InputGate](../../assets/img/boostcamp/InputGate.png)


* Generate hidden state by passing cell state to tanh and output gate

* Pass this hidden state to next time step, and ouput or next layer if needed 

    * $o_{t}=\sigma(W_{o}\cdot[h_{t-1},x_{t}]+b_{o})$

    * $h_{t}=o_{t}\cdot tanh(C_{t})$

![image-OutputGate](../../assets/img/boostcamp/OutputGate.png)

---

### Gated Recurrent Unit(GRU)

![image-GRU](../../assets/img/boostcamp/GRU.png)


