---
title:  "Day16 학습정리"
excerpt: "DL Basic"

categories:
  - Blog
tags:
  - [Blog, jekyll, Github, Git]
use_math: true
toc: true
toc_sticky: true
 
date: 2021-02-15
last_modified_at: 2021-02-20
---

# day16 학습정리

## Intro to Natural Language Processing(NLP)

* Natural Language Processing

    * Low level parsing 

        * Tokenization 

            주어진 copus에서 토큰이라 불리는 단위로 나누는 작업. 토큰의 단위는 상황에 따라 다르지만 주로 의미있는 단위로 토큰을 정한다.

        * stemming 

            어간추출이라 불리는 stemming은 형태학적 분석을 단순화한 것으로 볼 수있고 정해진 규칙만 보고 단어의 어미를 자는 어림짐작 작업이다. 따라서 섬세한 작업이 아니기 때문에 어간 추출 후 나오는 단어는 사전에 존재하기 않을 수 있다.

    * Word and phrase level

        * Named entity recognition(NER)

            "The New York Times"와 같이 분해해서는 안되는 고유명사같은 것을 찾는 작업.

        * POS-tagging 

            성분분석. 주어, 동사, 목적어, 부사구, 형용사구, 형용사구가 어떤 명사구를 수식하는지를 인식.

    * Sentence level

        * Sentiment analysis : 감정분석

        * Machine translation : 기계번역 (영어 -> 한국어)등

    * Multi-sentence and paragraph level

        * Entailment prediction

            두 문장간의 내포, 문장간의 논리적 관계를 예측

        * Question answering 

            질문의 키워드를 검색. 검색한 문서를 기반으로 독해를 통해 정답을 유추

        * Dialog system : 챗봇. 대화를 수행할 수 있는 기술

        * Summarization : 뉴스와 같은 글을 한 줄 요약함. (등등)


---

* Text mining : 빅데이터 분석과 관련. 

    ex) 과거 1년동안의 수만건의 뉴스기사를 모아 거기서 부터 나타나는 특정키워드의 빈도수를 시간순으로 나열하여 트렌드를 분석. 특정 유명인의 이미지를 분석.<br/>
    <br/>
    ex) 회사의 상품이 출시 되었을 때, 소비자 반응을 읽어냄

---

### Trends of NLP

* Text data can basically be viewed as a sequence of words, and each word can be represented as a vector through a technique such as Word2Vec or GloVe.

* RNN-family models(LSTM and GRUs), which take the sequence of these vectors of words as input, are the main architecuter of NLP tasks.

* Overall performance of NLP tasks has been improved since atteion modules and Transformer models, which replaced RNNs with self-atteion, have been inroduced a few years ago.

* As is the case ofr Transformer models, most of the advanced NLP models have been originally developed for improving machine translation task.


---

### Bag-of Words Representaion

step1. Constructing the vocabulary containing unique words.<br/>

step2. Encoding unique words to one-hot vectors.<br/>

* A sentence/document can be represented as the sum of one-hot vectors.

---

### NaiveBayes Classifier for Document Classification

* Bag-of-Words for Document Classification

![image-BayesProb](../../assets/img/boostcamp/BayesProb.png)

* For a document d,which consists of a sequence of word w, and a class c

* The Probability of a document can be represented by multiplying the probability of each word appearing.

* $P(d|c)P(c)=P(w_1,w_2,...,w_n|c)P(c)$ (by conditional independence assumption)


* [NaiveBayesClassifier 실습](https://github.com/kyungminkim-dev/boostcamp-ai-tech/blob/main/Week4(NLP)/day16/1_naive_bayes.ipynb){: target="_blank"}

* [Word2Vec 실습](https://github.com/kyungminkim-dev/boostcamp-ai-tech/blob/main/Week4(NLP)/day16/2_word2vec.ipynb){: target="_blank"}
    
